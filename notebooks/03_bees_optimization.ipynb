{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "585517cc",
   "metadata": {},
   "source": [
    "\n",
    "# 03 · Test Assembly via Bees Algorithm (24 / 48 items)\n",
    "\n",
    "This notebook assembles test forms using a **Bees Algorithm** metaheuristic.\n",
    "The objective is to **maximize measurement quality** under simple content constraints.\n",
    "\n",
    "**Objective (multi-criteria):**\n",
    "We use a blended score for a candidate item set:\n",
    "- `alpha`: Cronbach's alpha (internal consistency)\n",
    "- `pc1_var`: variance explained by the first principal component of the item matrix\n",
    "\n",
    "Objective = `w_alpha * alpha + w_pc1 * pc1_var` (default: 0.6 / 0.4)\n",
    "\n",
    "**Constraints:**\n",
    "- Target length: 24 or 48 items\n",
    "- Optional content balance by `scale_id`: minimal and maximal items per scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try importing alpha from your module, else local fallback\n",
    "try:\n",
    "    from src.cleaning_psych import cronbach_alpha as _alpha\n",
    "except Exception:\n",
    "    def _alpha(df):\n",
    "        X = df.to_numpy(dtype=float)\n",
    "        col_means = np.nanmean(X, axis=0, keepdims=True)\n",
    "        inds = np.where(np.isnan(X))\n",
    "        if inds[0].size:\n",
    "            X[inds] = np.take(col_means, inds[1])\n",
    "        k = X.shape[1]\n",
    "        if k < 2:\n",
    "            return np.nan\n",
    "        var_items = X.var(axis=0, ddof=1)\n",
    "        var_total = X.sum(axis=1).var(ddof=1)\n",
    "        if var_total == 0:\n",
    "            return np.nan\n",
    "        return float((k/(k-1)) * (1 - var_items.sum()/var_total))\n",
    "\n",
    "def pc1_variance_explained(df):\n",
    "    \"\"\"Proportion of variance explained by PC1 on column-standardized data.\"\"\"\n",
    "    X = df.to_numpy(dtype=float)\n",
    "    # impute column means\n",
    "    col_means = np.nanmean(X, axis=0, keepdims=True)\n",
    "    inds = np.where(np.isnan(X))\n",
    "    if inds[0].size:\n",
    "        X[inds] = np.take(col_means, inds[1])\n",
    "    # standardize columns\n",
    "    X = (X - X.mean(axis=0, keepdims=True))\n",
    "    col_std = X.std(axis=0, ddof=1, keepdims=True)\n",
    "    col_std[col_std == 0] = 1.0\n",
    "    X = X / col_std\n",
    "    # SVD\n",
    "    U, S, VT = np.linalg.svd(X, full_matrices=False)\n",
    "    var = (S**2)\n",
    "    return float(var[0] / var.sum()) if var.sum() > 0 else 0.0\n",
    "\n",
    "def objective(subset_cols, wide_scored, w_alpha=0.6, w_pc1=0.4):\n",
    "    if len(subset_cols) < 3:\n",
    "        return -np.inf, {\"alpha\": np.nan, \"pc1_var\": np.nan}\n",
    "    df = wide_scored[subset_cols]\n",
    "    a = _alpha(df)\n",
    "    p = pc1_variance_explained(df)\n",
    "    score = w_alpha * a + w_pc1 * p\n",
    "    return score, {\"alpha\": a, \"pc1_var\": p}\n",
    "\n",
    "def content_ok(subset_cols, item_stats, min_per_scale=None, max_per_scale=None):\n",
    "    \"\"\"Check simple content constraints per scale_id.\"\"\"\n",
    "    if min_per_scale is None and max_per_scale is None:\n",
    "        return True\n",
    "    scales = item_stats.set_index(\"item_id\").reindex(subset_cols)[\"scale_id\"]\n",
    "    counts = scales.value_counts(dropna=False).to_dict()\n",
    "    if min_per_scale:\n",
    "        for sc, m in min_per_scale.items():\n",
    "            if counts.get(sc, 0) < m:\n",
    "                return False\n",
    "    if max_per_scale:\n",
    "        for sc, M in max_per_scale.items():\n",
    "            if counts.get(sc, 0) > M:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def random_feasible_set(all_items, item_stats, k, min_per_scale=None, max_per_scale=None, max_tries=5000):\n",
    "    \"\"\"Sample a feasible set honoring content constraints.\"\"\"\n",
    "    items = list(all_items)\n",
    "    for _ in range(max_tries):\n",
    "        cand = random.sample(items, k)\n",
    "        if content_ok(cand, item_stats, min_per_scale, max_per_scale):\n",
    "            return cand\n",
    "    # fallback: return any subset (may violate constraints)\n",
    "    return random.sample(items, k)\n",
    "\n",
    "def neighborhood_swap(current, all_items, n_swaps=1):\n",
    "    \"\"\"Generate a neighbor by swapping n items with the pool.\"\"\"\n",
    "    current_set = set(current)\n",
    "    pool = list(set(all_items) - current_set)\n",
    "    new = current.copy()\n",
    "    for _ in range(n_swaps):\n",
    "        if not new or not pool:\n",
    "            break\n",
    "        out_idx = random.randrange(len(new))\n",
    "        in_idx = random.randrange(len(pool))\n",
    "        new[out_idx], pool[in_idx] = pool[in_idx], new[out_idx]\n",
    "    return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "ITEM_STATS_PATH = \"output/item_stats.csv\"\n",
    "WIDE_SCORED_PATH = \"output/wide_scored.csv\"\n",
    "\n",
    "assert os.path.exists(ITEM_STATS_PATH), f\"Missing {ITEM_STATS_PATH}\"\n",
    "assert os.path.exists(WIDE_SCORED_PATH), f\"Missing {WIDE_SCORED_PATH}\"\n",
    "\n",
    "item_stats = pd.read_csv(ITEM_STATS_PATH)\n",
    "wide_scored = pd.read_csv(WIDE_SCORED_PATH, index_col=0)\n",
    "\n",
    "# Ensure item_id column exists\n",
    "assert \"item_id\" in item_stats.columns, \"item_stats must include 'item_id'\"\n",
    "assert \"scale_id\" in item_stats.columns, \"item_stats must include 'scale_id'\"\n",
    "\n",
    "all_items = [c for c in wide_scored.columns if c in set(item_stats[\"item_id\"])]\n",
    "print(f\"Candidate items: {len(all_items)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6404462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bees_algorithm(\n",
    "    all_items,\n",
    "    item_stats,\n",
    "    wide_scored,\n",
    "    target_k=24,\n",
    "    min_per_scale=None,\n",
    "    max_per_scale=None,\n",
    "    iters=120,\n",
    "    n_scouts=20,\n",
    "    elite_sites=3,\n",
    "    best_sites=5,\n",
    "    recruits_elite=25,\n",
    "    recruits_best=10,\n",
    "    neighborhood_size=2,\n",
    "    seed=42,\n",
    "    w_alpha=0.6,\n",
    "    w_pc1=0.4,\n",
    "):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # 1) Initialize scouts (feasible sets)\n",
    "    population = []\n",
    "    for _ in range(n_scouts):\n",
    "        subset = random_feasible_set(all_items, item_stats, target_k, min_per_scale, max_per_scale)\n",
    "        score, metas = objective(subset, wide_scored, w_alpha, w_pc1)\n",
    "        population.append({\"subset\": subset, \"score\": score, **metas})\n",
    "\n",
    "    history = []\n",
    "    best_global = max(population, key=lambda x: x[\"score\"]).copy()\n",
    "\n",
    "    for it in range(iters):\n",
    "        # 2) Sort by score\n",
    "        population.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "        # 3) Neighborhood search (recruitment) around top sites\n",
    "        new_population = []\n",
    "        for idx, sol in enumerate(population[:elite_sites + best_sites]):\n",
    "            recruits = recruits_elite if idx < elite_sites else recruits_best\n",
    "            local_best = sol\n",
    "            for _ in range(recruits):\n",
    "                neighbor = neighborhood_swap(sol[\"subset\"], all_items, n_swaps=neighborhood_size)\n",
    "                if not content_ok(neighbor, item_stats, min_per_scale, max_per_scale):\n",
    "                    continue\n",
    "                s, metas = objective(neighbor, wide_scored, w_alpha, w_pc1)\n",
    "                if s > local_best[\"score\"]:\n",
    "                    local_best = {\"subset\": neighbor, \"score\": s, **metas}\n",
    "            new_population.append(local_best)\n",
    "\n",
    "        # 4) Random scouts to refill population\n",
    "        while len(new_population) < n_scouts:\n",
    "            subset = random_feasible_set(all_items, item_stats, target_k, min_per_scale, max_per_scale)\n",
    "            s, metas = objective(subset, wide_scored, w_alpha, w_pc1)\n",
    "            new_population.append({\"subset\": subset, \"score\": s, **metas})\n",
    "\n",
    "        population = new_population\n",
    "\n",
    "        # 5) Track best\n",
    "        current_best = max(population, key=lambda x: x[\"score\"]\n",
    "        )\n",
    "        if current_best[\"score\"] > best_global[\"score\"]:\n",
    "            best_global = current_best.copy()\n",
    "\n",
    "        history.append(best_global[\"score\"])\n",
    "\n",
    "    return best_global, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8027a32c",
   "metadata": {},
   "source": [
    "\n",
    "## Content balance\n",
    "\n",
    "You can set minimum / maximum items per `scale_id`.  \n",
    "If you don't need content constraints, leave both as `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951b08d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example policy: minimal presence of each scale that appears in the data.\n",
    "present_scales = item_stats[\"scale_id\"].dropna().unique().tolist()\n",
    "min_per_scale_24 = {sc: 1 for sc in present_scales}   # minimal presence\n",
    "max_per_scale_24 = None\n",
    "\n",
    "min_per_scale_48 = {sc: 2 for sc in present_scales}\n",
    "max_per_scale_48 = None\n",
    "\n",
    "print(\"Scales:\", present_scales[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d0e03",
   "metadata": {},
   "source": [
    "## Run Bees Algorithm — 24 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0208a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best24, hist24 = bees_algorithm(\n",
    "    all_items=all_items,\n",
    "    item_stats=item_stats,\n",
    "    wide_scored=wide_scored,\n",
    "    target_k=24,\n",
    "    min_per_scale=min_per_scale_24,\n",
    "    max_per_scale=max_per_scale_24,\n",
    "    iters=120,\n",
    "    n_scouts=20,\n",
    "    elite_sites=3,\n",
    "    best_sites=5,\n",
    "    recruits_elite=25,\n",
    "    recruits_best=10,\n",
    "    neighborhood_size=2,\n",
    "    seed=42,\n",
    "    w_alpha=0.6,\n",
    "    w_pc1=0.4\n",
    ")\n",
    "\n",
    "print(\"Best 24-form objective:\", best24[\"score\"])\n",
    "print(\"Alpha:\", best24[\"alpha\"], \"PC1 Var:\", best24[\"pc1_var\"])\n",
    "\n",
    "# Save selection\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "sel24 = pd.DataFrame({\"item_id\": best24[\"subset\"]}).merge(item_stats, on=\"item_id\", how=\"left\")\n",
    "sel24.to_csv(\"output/assembly_24.csv\", index=False)\n",
    "print(\"Saved: output/assembly_24.csv\")\n",
    "\n",
    "# Plot convergence\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(hist24)+1), hist24, marker=\"o\")\n",
    "plt.title(\"Bees Algorithm — Convergence (24 items)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537b3205",
   "metadata": {},
   "source": [
    "## Run Bees Algorithm — 48 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00e907",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best48, hist48 = bees_algorithm(\n",
    "    all_items=all_items,\n",
    "    item_stats=item_stats,\n",
    "    wide_scored=wide_scored,\n",
    "    target_k=48,\n",
    "    min_per_scale=min_per_scale_48,\n",
    "    max_per_scale=max_per_scale_48,\n",
    "    iters=140,\n",
    "    n_scouts=24,\n",
    "    elite_sites=3,\n",
    "    best_sites=6,\n",
    "    recruits_elite=30,\n",
    "    recruits_best=12,\n",
    "    neighborhood_size=2,\n",
    "    seed=123,\n",
    "    w_alpha=0.6,\n",
    "    w_pc1=0.4\n",
    ")\n",
    "\n",
    "print(\"Best 48-form objective:\", best48[\"score\"])\n",
    "print(\"Alpha:\", best48[\"alpha\"], \"PC1 Var:\", best48[\"pc1_var\"])\n",
    "\n",
    "sel48 = pd.DataFrame({\"item_id\": best48[\"subset\"]}).merge(item_stats, on=\"item_id\", how=\"left\")\n",
    "sel48.to_csv(\"output/assembly_48.csv\", index=False)\n",
    "print(\"Saved: output/assembly_48.csv\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(hist48)+1), hist48, marker=\"o\")\n",
    "plt.title(\"Bees Algorithm — Convergence (48 items)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b608064",
   "metadata": {},
   "source": [
    "## Compare 24 vs 48 forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute and print summary stats\n",
    "def form_summary(subset):\n",
    "    cols = [c for c in subset if c in wide_scored.columns]\n",
    "    df = wide_scored[cols]\n",
    "    a = _alpha(df)\n",
    "    p = pc1_variance_explained(df)\n",
    "    return {\"k\": len(cols), \"alpha\": a, \"pc1_var\": p}\n",
    "\n",
    "sum24 = form_summary(best24[\"subset\"])\n",
    "sum48 = form_summary(best48[\"subset\"])\n",
    "\n",
    "print(\"Summary 24:\", sum24)\n",
    "print(\"Summary 48:\", sum48)\n",
    "\n",
    "# Scale coverage\n",
    "def coverage(subset):\n",
    "    sc = item_stats.set_index(\"item_id\").reindex(subset)[\"scale_id\"]\n",
    "    return sc.value_counts(dropna=False).to_frame(\"count\")\n",
    "\n",
    "cov24 = coverage(best24[\"subset\"])\n",
    "cov48 = coverage(best48[\"subset\"])\n",
    "\n",
    "print(\"\n",
    "Coverage 24:\")\n",
    "print(cov24.sort_index())\n",
    "\n",
    "print(\"\n",
    "Coverage 48:\")\n",
    "print(cov48.sort_index())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
