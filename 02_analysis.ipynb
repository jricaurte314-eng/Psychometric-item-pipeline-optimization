{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d259bd",
   "metadata": {},
   "source": [
    "\n",
    "# 02 · Psychometric EDA & Reliability Analysis\n",
    "\n",
    "This notebook performs:\n",
    "- Item difficulty & discrimination distributions\n",
    "- Difficulty vs. discrimination scatter\n",
    "- Cronbach's alpha (recomputed from `wide_scored`)\n",
    "- Reliability curve (alpha vs. number of items, ordered by discrimination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6997ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: use your module if available to recompute alpha\n",
    "try:\n",
    "    from src.cleaning_psych import cronbach_alpha\n",
    "except Exception as e:\n",
    "    cronbach_alpha = None\n",
    "    print(\"Note: src.cleaning_psych not found; using local alpha implementation.\")\n",
    "\n",
    "    def cronbach_alpha(X_df):\n",
    "        X = X_df.to_numpy(dtype=float)\n",
    "        # Impute column means for NaNs\n",
    "        col_means = np.nanmean(X, axis=0, keepdims=True)\n",
    "        inds = np.where(np.isnan(X))\n",
    "        if inds[0].size:\n",
    "            X[inds] = np.take(col_means, inds[1])\n",
    "        k = X.shape[1]\n",
    "        if k < 2:\n",
    "            return np.nan\n",
    "        var_items = X.var(axis=0, ddof=1)\n",
    "        var_total = X.sum(axis=1).var(ddof=1)\n",
    "        if var_total == 0:\n",
    "            return np.nan\n",
    "        return float((k/(k-1)) * (1 - var_items.sum()/var_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ITEM_STATS_PATH = \"output/item_stats.csv\"\n",
    "RESP_SCORES_PATH = \"output/respondent_scores.csv\"\n",
    "WIDE_SCORED_PATH = \"output/wide_scored.csv\"\n",
    "\n",
    "assert os.path.exists(ITEM_STATS_PATH), f\"Missing {ITEM_STATS_PATH}. Re-run the first notebook.\"\n",
    "assert os.path.exists(RESP_SCORES_PATH), f\"Missing {RESP_SCORES_PATH}. Re-run the first notebook.\"\n",
    "assert os.path.exists(WIDE_SCORED_PATH), f\"Missing {WIDE_SCORED_PATH}. Re-run the first notebook.\"\n",
    "\n",
    "item_stats = pd.read_csv(ITEM_STATS_PATH)\n",
    "respondent_scores = pd.read_csv(RESP_SCORES_PATH, index_col=0)\n",
    "wide_scored = pd.read_csv(WIDE_SCORED_PATH, index_col=0)\n",
    "\n",
    "print(f\"Loaded item_stats: {item_stats.shape}\")\n",
    "print(f\"Loaded respondent_scores: {respondent_scores.shape}\")\n",
    "print(f\"Loaded wide_scored: {wide_scored.shape}\")\n",
    "item_stats.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3918d",
   "metadata": {},
   "source": [
    "## Difficulty distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9126895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "vals = item_stats[\"difficulty\"].dropna().to_numpy()\n",
    "plt.hist(vals, bins=20)\n",
    "plt.title(\"Item Difficulty\")\n",
    "plt.xlabel(\"Difficulty (mean score)\")\n",
    "plt.ylabel(\"Count of items\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97ee31",
   "metadata": {},
   "source": [
    "## Discrimination distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "vals = item_stats[\"discrimination\"].dropna().to_numpy()\n",
    "plt.hist(vals, bins=20)\n",
    "plt.title(\"Item Discrimination (item–total corrected)\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Count of items\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cc483",
   "metadata": {},
   "source": [
    "## Difficulty vs Discrimination (scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "x = item_stats[\"difficulty\"].to_numpy()\n",
    "y = item_stats[\"discrimination\"].to_numpy()\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Difficulty vs Discrimination\")\n",
    "plt.xlabel(\"Difficulty (mean score)\")\n",
    "plt.ylabel(\"Discrimination (corr item–total corrected)\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e3c03",
   "metadata": {},
   "source": [
    "## Cronbach's alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha_val = cronbach_alpha(wide_scored)\n",
    "print(f\"Cronbach's alpha (all items): {alpha_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88fd73",
   "metadata": {},
   "source": [
    "## Reliability curve (alpha vs number of items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e24522",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Order items by discrimination descending; remove NaNs first\n",
    "ordered = item_stats.dropna(subset=[\"discrimination\"]).sort_values(\"discrimination\", ascending=False)\n",
    "ordered_items = ordered[\"item_id\"].tolist()\n",
    "\n",
    "alphas = []\n",
    "ks = []\n",
    "for k in range(3, len(ordered_items)+1):  # start from 3 items to avoid degenerate alpha\n",
    "    subset_cols = [c for c in ordered_items[:k] if c in wide_scored.columns]\n",
    "    if len(subset_cols) < 3:\n",
    "        continue\n",
    "    alpha_k = cronbach_alpha(wide_scored[subset_cols])\n",
    "    ks.append(k)\n",
    "    alphas.append(alpha_k)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ks, alphas, marker=\"o\")\n",
    "plt.title(\"Reliability Curve: Alpha vs Number of Items (ranked by discrimination)\")\n",
    "plt.xlabel(\"Number of items\")\n",
    "plt.ylabel(\"Cronbach's alpha\")\n",
    "plt.grid(True, linestyle=\"--\", linewidth=0.5, alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129b670",
   "metadata": {},
   "source": [
    "## Item quality flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Heuristic flags (adjust thresholds to your scale):\n",
    "# - Low discrimination < 0.15\n",
    "# - Extreme difficulty (too easy/hard) outside [0.2, 0.8] for binary or scaled mean bounds for Likert\n",
    "flags = item_stats.copy()\n",
    "flags[\"flag_low_disc\"] = flags[\"discrimination\"] < 0.15\n",
    "flags[\"flag_easy\"] = flags[\"difficulty\"] > 0.8\n",
    "flags[\"flag_hard\"] = flags[\"difficulty\"] < 0.2\n",
    "flags[\"any_flag\"] = flags[[\"flag_low_disc\",\"flag_easy\",\"flag_hard\"]].any(axis=1)\n",
    "\n",
    "flags_sorted = flags.sort_values([\"any_flag\", \"discrimination\"], ascending=[False, True])\n",
    "\n",
    "# Show top 15 flagged\n",
    "flags_sorted.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc28ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report_cols = [\"item_id\",\"scale_id\",\"difficulty\",\"discrimination\",\"flag_low_disc\",\"flag_easy\",\"flag_hard\",\"any_flag\"]\n",
    "report = flags_sorted[report_cols]\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "report_path = \"output/item_quality_report.csv\"\n",
    "report.to_csv(report_path, index=False)\n",
    "print(f\"Saved: {report_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
